# Universal Transformer Chatbot ðŸ¤–

This chatbot is built using the Universal Transformer model, coded from scratch, which extends the original Transformer with recurrence and dynamic depth. Itâ€™s based on the paper ["Universal Transformers"](https://arxiv.org/abs/1807.03819) by Google Brain.

> ðŸ§  This work is inspired by Google's Universal Transformer architecture, which improves generalization through shared layers and adaptive computation.

<p align="center">
  <img src="Screenshot 2025-07-03 031945.png" alt="Sample Gesture" width="600"/>
</p>



## ðŸ”§ Model Overview
The model:
- Applies the same Transformer block recurrently
- Supports **adaptive computation time** (optional)
- Shares parameters across steps for efficiency

### Key Parameters
- `d_model`: Embedding size 
- `n_heads`: Attention heads
- `n_steps`: Recurrence steps (layer depth)
- `use_act`: Enable ACT (dynamic halting)

## ðŸ§ª Pipeline
1. **Preprocessing** â€“ Clean, tokenize, and pad sentences
2. **Training** â€“ Recurrent layer application with shared weights
3. **Inference** â€“ Predict sequences step-by-step

## ðŸ›  Requirements
- Python 3.8+
- PyTorch
- NumPy
- Jupyter Notebook

## ðŸš€ Usage
Run `universal_transformer_chatbot.ipynb` to train and interact with the chatbot.

---

